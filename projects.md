---
layout: page
title: Projects
subtitle: Ongoing and completed work
published: true
---
Check out my [Google Scholar](https://scholar.google.com/citations?user=t1hINkMAAAAJ&hl=en) profile.

### **Data augmentation and style transfer for prosthesis control using deep learning** 

#### *Motivation*:
Test
#### *Approach*:
Test

### **Open access human movement data for improving lower-limb wearable robotics**
<img style="float: center;" src="http://blair-hu.github.io/img/OpenSourceDataset1.jpg" width="500">

#### *Motivation*: 
Benchmark datasets containing high-resolution, device-agnostic neuromechanical signals from unimpaired individuals are critical to helping the wearable robotics community objectively evaluate device performance for a variety of walking-related activities. In addition, rich labeled training data from wearable sensors is valuable for developing activity prediction algorithms. Our contribution, the ENcyclopedia of Able-Bodied Lower-Limb Locomotor Signals (ENABL3S), represents a "best case scenario" (i.e. the device is fully transparent and does not impede movement) for activity prediction and is intended to fill a gap in the field by making this type of data publicly available. 

#### *Approach*:
We used wearable sensors to collect bilateral myoelectric and kinematic signals (52 channels, 1 kHz) from the lower extremities of ten healthy human subjects as they spontaneously transitioned between locomotor activities including level ground walking and ascending/descending stairs and ramps. The experimenter labeled the ground truth with a key fob. We used a temporal raster plot to highlight different patterns in the neuromechanical signals corresponding to different locomotor activities. The unstructured data were cleaned, organized, validated against results aggregated from previously published biomechanics studies, and hosted on Figshare.

[Code](https://github.com/blair-hu/OS-Data) | [Data Repository](https://doi.org/10.6084/m9.figshare.5362627) | [Paper](http://blair-hu.github.io/files/BHu_OpenSourceDataset_Frontiers2018.pdf)

### **Bilateral neuromechanical sensor fusion for intent recognition**
<img style="float: center;" src="http://blair-hu.github.io/img/BilateralIntentRecognition1.png" width="500">

#### *Motivation*:
Walking requires interlimb coordination because leading and trailing legs have distinct biomechanical functions, especially for amputee subjects who typicalyl exhibit exaggerated gait assymetries. However, state of the art intent recognition (predicting future states using movement-related signals detected before movement completion) algorithms have not been systematically applied to bilateral control signals. We tested the hypothesis that including information from both legs would improve accuracy over a controller using unilateral information alone.

#### *Approach*:
Test 

[Code](https://github.com/blair-hu/Bilateral-IR) | [Paper](http://blair-hu.github.io/files/BHu_BilateralIntentRecognition_Frontiers2018.pdf)

### **Bilateral gait segmentation using a single sensor**
<img style="float: center;" src="http://blair-hu.github.io/img/BilateralGaitSegmentation2.png" width="500">

#### *Motivation*:
Test
#### *Approach*:
Test

[Code](https://github.com/blair-hu/Bilateral-GS) | [Paper](http://blair-hu.github.io/files/BHu_BilateralGaitSegmentation_BioRob2018.pdf)
